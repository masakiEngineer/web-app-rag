import os
import tempfile
from dotenv import load_dotenv
import streamlit as st

from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain  # â†ã“ã“ã‚’å¤‰æ›´
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
def load_api_key() -> str:
    """
    .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ OpenAI API ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€

    Returns:
        str: APIã‚­ãƒ¼
    """
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("`.env` ã« `OPENAI_API_KEY` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    return api_key

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰² ---
def load_and_split_documents(uploaded_file, chunk_size: int = 500, chunk_overlap: int = 50) -> list:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²

    Args:
        uploaded_file: Streamlit ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        chunk_size (int): å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚µã‚¤ã‚º
        chunk_overlap (int): ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

    Returns:
        list: åˆ†å‰²ã•ã‚ŒãŸ Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    loader = TextLoader(tmp_path, encoding='utf-8')
    docs = loader.load()
    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_documents(docs)

# --- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¨ConversationalRetrievalChainã®ä½œæˆ ---
def build_qa_chain(docs: list, api_key: str, k: int = 3):
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«DBã¨ConversationalRetrievalChainã‚’æ§‹ç¯‰

    Args:
        docs (list): Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
        api_key (str): OpenAI APIã‚­ãƒ¼
        k (int): æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹ä¸Šä½æ–‡æ›¸æ•°

    Returns:
        ConversationalRetrievalChain: ä¼šè©±å½¢å¼QAãƒã‚§ãƒ¼ãƒ³
    """
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vector_db = FAISS.from_documents(docs, embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": k})
    llm = ChatOpenAI(openai_api_key=api_key)
    return ConversationalRetrievalChain.from_llm(llm, retriever)

# --- ãƒãƒ£ãƒƒãƒˆUIã®è¡¨ç¤ºã¨å‡¦ç† ---
def run_chat_ui(qa_chain):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ãƒãƒ£ãƒƒãƒˆUIå‡¦ç†ã‚’è¡Œã†

    Args:
        qa_chain: ConversationalRetrievalChainã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ä¼šè©±å±¥æ­´ã‚’ç”»é¢ã«è¡¨ç¤º
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        st.session_state.chat_history.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            pairs = []
            messages = st.session_state.chat_history
            length = len(messages)
            if length % 2 != 0:
                length -= 1
            for i in range(0, length, 2):
                pairs.append((messages[i]["content"], messages[i+1]["content"]))

            result = qa_chain({"question": prompt, "chat_history": pairs})
            response = result["answer"]
            st.markdown(response)

        st.session_state.chat_history.append({"role": "assistant", "content": response})

        with st.expander("ğŸ“š ä½¿ç”¨ã•ã‚ŒãŸæ–‡æ›¸"):
            for idx, doc in enumerate(result.get("source_documents", [])):
                st.markdown(f"**{idx+1}.** {doc.page_content[:300]}...")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    api_key = load_api_key()
    st.set_page_config(page_title="RAGãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª", layout="wide")

    st.markdown("""
        <style>
            body { background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); font-family: 'Segoe UI', sans-serif; }
            .main { background-color: rgba(255, 255, 255, 0.85); border-radius: 20px; padding: 30px; box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.2); }
            h1 { text-align: center; color: #ff5e62; font-size: 48px; margin-bottom: 20px; }
        </style>
    """, unsafe_allow_html=True)

    st.markdown("<h1> RAG ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª</h1>", unsafe_allow_html=True)
    st.markdown('<div class="main">', unsafe_allow_html=True)

    if api_key:
        uploaded_file = st.file_uploader("ğŸ”½ æ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["txt", "md", "pdf"])
        if uploaded_file:
            docs = load_and_split_documents(uploaded_file)
            qa_chain = build_qa_chain(docs, api_key)
            run_chat_ui(qa_chain)
        else:
            st.info("ğŸ” ã¾ãšã¯æ¤œç´¢å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

    st.markdown("</div>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()